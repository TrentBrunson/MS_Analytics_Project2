{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3.97\n",
    "\n",
    "# Dependencies\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def main():\n",
    "    makeDir\n",
    "    dataFrame = dataLoad\n",
    "    eda(dataFrame)\n",
    "    viz(dataFrame)\n",
    "    newDF = dataCleanTransform(dataFrame)\n",
    "\n",
    "    ''' Comment out for now'''\n",
    "    # # This library captures many EDA elements\n",
    "    # # https://pandas-profiling.github.io/pandas-profiling/docs/master/index.html\n",
    "    # profile = ProfileReport(dataFrame, title=\"Cardiovascular Disease Data Profiling Report\", explorative=True)\n",
    "    # # export for report\n",
    "    # profile.to_file(\"CVD.html\")\n",
    "    # # print to screen in interactive frame\n",
    "    # # 3.1 widgets() is broken; regress to 3.0 for widgets or use to_notebook_iframe (CAO: Nov 2021)\n",
    "    # # Selecting alternative path since user action required to rollback version\n",
    "    # profile.to_notebook_iframe()\n",
    "    '''end comment'''\n",
    "    \n",
    "    model(newDF)\n",
    "    cleanedDataFile(newDF)     \n",
    "    # run other notebooks in library\n",
    "    # %run ./eda.ipynb\n",
    "    # %run ./models.ipynb\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDir():\n",
    "    path1 = \"images\"  # make a location to store images\n",
    "    path2 = \"data\"  # make a location to store images\n",
    "    # mkdir for images\n",
    "    try:\n",
    "        os.mkdir(path1)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed; it already exists.\" % path1)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s\" % path1)\n",
    "    # mkdir for cleaned data file\n",
    "    try:\n",
    "        os.mkdir(path2)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed; it already exists.\" % path2)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s\" % path2)\n",
    "\n",
    "def dataLoad():\n",
    "    file = 'data\\heart.csv'\n",
    "    df = pd.read_csv(file)\n",
    "    return df\n",
    "\n",
    "def cleanedDataFile(df):\n",
    "    csv_path = f\"data\\heartCleaned.csv\"\n",
    "    df.to_csv(csv_path, sep= ',', index= False)\n",
    "    print(f\"\\n\\nCleaned data is saved to: \\n{csv_path}\")\n",
    "\n",
    "def eda(df):\n",
    "    df.describe()\n",
    "    df.info()\n",
    "    # check for dupes\n",
    "    print('Number of Duplicates:', len(df[df.duplicated()]))\n",
    "    df.count()\n",
    "    # check for dupe rows\n",
    "    df.value_counts()\n",
    "    df.notna()\n",
    "    # examine unique values; look for binary variables and other non-continuous data\n",
    "    df.nunique()\n",
    "    # checking for NaN values\n",
    "    df.isna().sum()\n",
    "    # count for whole dataframe\n",
    "    df.isna().sum().sum()\n",
    "    df.corr()\n",
    "\n",
    "def viz(df):\n",
    "    # Look for zeroes in datasets that may be unreasonable.\n",
    "    fig, axs = plt.subplots(len(df.columns), figsize=(5, 20))\n",
    "    for n, col in enumerate(df.columns):\n",
    "        # print(col)\n",
    "        a = df[col].hist(ax=axs[n])\n",
    "        a.set_title(col)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # cholesterol stood out with about 18% of the values being 0 - visualize\n",
    "    df.Cholesterol.plot(kind = \"hist\", bins = 20, figsize = (8,5))\n",
    "    plt.legend()\n",
    "    plt.savefig(\"images\\cholesterol.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Viz for title page \n",
    "    plt.figure(figsize=(20,20))\n",
    "    sns.displot(df['Age'], color=\"red\", label=\"Age\", kde= True)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"images\\CVD_by_Age.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # look at correlations to target\n",
    "    targetHeatmap = sns.heatmap(\n",
    "        df.corr()[['HeartDisease']].sort_values(by='HeartDisease', ascending=False), \n",
    "        vmin=-0.5, vmax=1, annot=True, cmap='BrBG', fmt = '.2f'\n",
    "        )\n",
    "    fig = targetHeatmap.get_figure()\n",
    "    fig.savefig('images\\heartdiseaseCorr.png')\n",
    "    fig\n",
    "\n",
    "    # Look for outliers in numeric variables\n",
    "    col = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "    num_col = df[col]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize =(10, 10))\n",
    "    fig.patch.set_facecolor('#f1faee')\n",
    "\n",
    "    c = 1\n",
    "    for i in num_col.columns: \n",
    "        plt.subplot(3, 2, c)\n",
    "        plt.boxplot(num_col[i])\n",
    "        plt.title(i, fontsize=12, color='Black')\n",
    "        plt.suptitle('Numeric Variables', fontsize=20)\n",
    "        c = c + 1\n",
    "    fig.savefig('images\\outliers.png')\n",
    "    \n",
    "    # check categorical variables\n",
    "    heart_df = df[df['HeartDisease'] == 1]\n",
    "    fig, ax = plt.subplots(figsize =(10, 10))\n",
    "    fig.patch.set_facecolor('#f1faee')\n",
    "    fig.savefig('images\\categoricalHistograms.png')\n",
    "\n",
    "    j = 1\n",
    "    for i in heart_df.columns: \n",
    "        if heart_df[i].dtypes  == 'object':\n",
    "            plt.subplot(3, 2, j)\n",
    "            sns.countplot(y = heart_df[i], data = heart_df, order=heart_df[i].value_counts().index, palette='Blues_r', edgecolor='black')\n",
    "            #plt.title(i, fontsize=15, color='black')\n",
    "            plt.suptitle('Categorical Variables', fontsize=20)\n",
    "            j = j + 1\n",
    "\n",
    "def dataCleanTransform(df):\n",
    "    # through visualization found 1 zero value for blood pressure = dead\n",
    "    # impute mean value for 0 in the resting BP with the mean for that demographic\n",
    "    df.RestingBP.mask(df.RestingBP == 0, 132, inplace=True)\n",
    "    cleanedDF = df\n",
    "    # encode categorical variables\n",
    "    categorical = ['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope']\n",
    "    encoded = pd.get_dummies(df[categorical])\n",
    "    print(encoded.info())\n",
    "    print(f\"\\n\\nNew, cleaned data stats: \\n\"\n",
    "        f\"{cleanedDF.describe()}\")\n",
    "    return cleanedDF\n",
    "\n",
    "def model(df):\n",
    "    # check counts for heart disease or not, and is it balanced or not?\n",
    "    round(df['HeartDisease'].value_counts()/len(df['HeartDisease'])*100)\n",
    "\n",
    "    # Dummy variables\n",
    "    pd.set_option('display.max_columns', 40)\n",
    "\n",
    "    # Select categorical variables\n",
    "    cat = df.select_dtypes(include=object).columns\n",
    "    df_dummy = pd.get_dummies(df, columns=cat)\n",
    "\n",
    "    inp = df_dummy.drop(columns='HeartDisease')\n",
    "    out = df_dummy['HeartDisease']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        inp, out, test_size=0.20, random_state=20)\n",
    "    \n",
    "    # Log regression\n",
    "    log_reg = LogisticRegression(solver='liblinear').fit(x_train, y_train)\n",
    "    y_pred_lr = log_reg.predict(x_test)\n",
    "    print('Accuracy score: ', round(accuracy_score(y_test, y_pred_lr), 3))\n",
    "    print('F1 Score: ', round(f1_score(y_test, y_pred_lr), 3))\n",
    "\n",
    "    # input, output var declarations \n",
    "    Y = df_dummy['HeartDisease']\n",
    "    X = df_dummy[['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']]\n",
    "    columns = X.columns\n",
    "    # apply Standardization\n",
    "    scaler = StandardScaler()\n",
    "    X_std = scaler.fit_transform(X)\n",
    "    X_std = pd.DataFrame(X_std, columns = columns)\n",
    "    # Dataset of all dummy columns\n",
    "    df2 = df_dummy.iloc[:,7:]\n",
    "    # Merge the Standardization column with dummy columns\n",
    "    X_nr = X_std.join(df2)\n",
    "\n",
    "    # train test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        X_nr, Y, test_size=0.20, random_state=20)\n",
    "    \n",
    "    log_reg_nr = LogisticRegression(solver='liblinear').fit(x_train, y_train)\n",
    "    y_pred_lr_nr = log_reg_nr.predict(x_test)\n",
    "    print('Accuracy score: ', round(accuracy_score(y_test, y_pred_lr_nr), 3))\n",
    "    print('F1 Score: ', round(f1_score(y_test, y_pred_lr_nr), 3))\n",
    "\n",
    "    # Confusion Matrix for Logistic RegressionÂ¶\n",
    "    predictedLabels: np.ndarray = log_reg_nr.predict(x_test)\n",
    "    confusionMatrixDF: pd.DataFrame = pd.DataFrame(metrics.confusion_matrix(y_test,predictedLabels),index=['Actual +','Actual -'],columns=['Predicted +', 'Predicted -'])\n",
    "    print(confusionMatrixDF)\n",
    "    print(metrics.classification_report(y_test, y_pred_lr))\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(random_state=20, n_estimators=100)\n",
    " \n",
    "    # x_train, x_test, y_train, y_test\n",
    "    rf = rf.fit(x_train, y_train)\n",
    "    y_pred_rf = rf.predict(x_test)\n",
    "\n",
    "    print('Accuracy score: ', round(accuracy_score(y_test, y_pred_rf), 3))\n",
    "    print('F1 Score: ', round(f1_score(y_test, y_pred_rf), 3))\n",
    "\n",
    "    # Tune the model with grid search\n",
    "    n_estimators = [100, 200, 300]\n",
    "    max_depth = [10, 20, 30]\n",
    "    max_depth.append(None)\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    min_samples_split = [5, 10, 15]\n",
    "    min_samples_leaf = [1, 2]\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    params = {'n_estimators': n_estimators, 'max_features': max_features,\n",
    "            'max_depth': max_depth, 'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\n",
    "\n",
    "    RF = RandomForestClassifier(random_state=20)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = RF, \n",
    "                            param_grid = params,\n",
    "                            scoring = 'f1',\n",
    "                            cv = 5,\n",
    "                            verbose=0, \n",
    "                            n_jobs=-1)\n",
    "\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    print(\"best score: \", grid_search.best_score_)\n",
    "    print(\"best param: \", grid_search.best_params_)\n",
    "\n",
    "    # apply best params to the RF model\n",
    "    best_para = grid_search.best_params_\n",
    "    rf_2 = RandomForestClassifier(random_state=20, **best_para)\n",
    "                                \n",
    "    rf_2 = rf_2.fit(x_train, y_train)\n",
    "    y_pred_rf_2 = rf_2.predict(x_test)\n",
    "    print(f'\\nReport on Ramdom Forest\\n')\n",
    "    print('Accuracy score: ', round(accuracy_score(y_test, y_pred_rf_2), 3))\n",
    "    print('F1 Score: ', round(f1_score(y_test, y_pred_rf_2), 3))\n",
    "\n",
    "    # Confusion matrix and report for RF\n",
    "    print(metrics.confusion_matrix(y_test, y_pred_rf_2))\n",
    "    print(metrics.classification_report(y_test, y_pred_rf_2))\n",
    "\n",
    "    # predictedLabels: np.ndarray = log_reg_nr.predict(x_test)\n",
    "    confusionMatrixDF: pd.DataFrame = pd.DataFrame(metrics.confusion_matrix(y_test,y_pred_rf_2),index=['Actual +','Actual -'],columns=['Predicted +', 'Predicted -'])\n",
    "    print(confusionMatrixDF)\n",
    "    print(metrics.classification_report(y_test, y_pred_rf_2))\n",
    "\n",
    "    # See what SKlearn says are important features with dummy variables in model\n",
    "    from sklearn.feature_selection import mutual_info_classif\n",
    "    plt.figure(figsize=(15,8))\n",
    "    imp = mutual_info_classif(inp, out)\n",
    "    feature_imp = pd.Series(imp, df_dummy.columns[0:len(df_dummy.columns)-1])\n",
    "    feature_imp = feature_imp.sort_values(ascending=True)\n",
    "    feature_imp.plot(kind = 'barh', color = 'teal')\n",
    "    plt.title(\"Feature importance plot\")\n",
    "    plt.savefig(\"images\\\\featureImportance.png\")\n",
    "\n",
    "    # See impact of using only the top 10 features\n",
    "    best_feat_df = df_dummy[['ST_Slope_Flat','Sex_M','ST_Slope_Down','RestingECG_ST','ExerciseAngina_N', \n",
    "                            'Oldpeak','ChestPainType_ASY','MaxHR','Sex_F','Cholesterol','HeartDisease']]\n",
    "\n",
    "    inp_feat = best_feat_df.drop(columns='HeartDisease')\n",
    "    out_feat = best_feat_df['HeartDisease']\n",
    "\n",
    "    x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(\n",
    "        inp_feat, out_feat, test_size=0.20, random_state=20)\n",
    "    \n",
    "    rf_feat = RandomForestClassifier(random_state=20, n_estimators=100)\n",
    " \n",
    "    rf_feat = rf_feat.fit(x_train_2, y_train_2)\n",
    "    y_pred_rf_feat = rf_feat.predict(x_test_2)\n",
    "    print(f'\\nReport on Ramdom Forest with only top 10 feautres\\n')\n",
    "    print('Accuracy score: ', round(accuracy_score(y_test_2, y_pred_rf_feat), 3))\n",
    "    print('F1 Score: ', round(f1_score(y_test_2, y_pred_rf_feat), 3))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
