{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDir():\n",
    "    path1 = \"images\"  # make a location to store images\n",
    "    path2 = \"data\"  # make a location to store images\n",
    "    # mkdir for images\n",
    "    try:\n",
    "        os.mkdir(path1)\n",
    "    except OSError:\n",
    "        print (\"\\nCreation of the directory %s failed; it already exists.\\n\" % path1)\n",
    "    else:\n",
    "        print (\"\\nSuccessfully created the directory %s\\n\" % path1)\n",
    "    # mkdir for cleaned data file\n",
    "    try:\n",
    "        os.mkdir(path2)\n",
    "    except OSError:\n",
    "        print (\"\\nCreation of the directory %s failed; it already exists.\\n\" % path2)\n",
    "    else:\n",
    "        print (\"\\nSuccessfully created the directory %s\\n\" % path2)\n",
    "\n",
    "def dataLoad():\n",
    "    file = 'data\\heart.csv'\n",
    "    df = pd.read_csv(file)\n",
    "    return df\n",
    "\n",
    "def cleanedDataFile(df):\n",
    "    csv_path = f\"data\\heartCleaned.csv\"\n",
    "    df.to_csv(csv_path, sep= ',', index= False)\n",
    "    print(f\"\\n\\nCleaned data is saved to: \\n{csv_path}\\n\")\n",
    "\n",
    "def eda(df):\n",
    "    print(f\"Basic stats on data:\\n{df.describe()}\\n\")\n",
    "    print(f\"Data info:\\n{df.info()}\\n\")\n",
    "    # check for dupes\n",
    "    print('\\nNumber of Duplicates:\\n', len(df[df.duplicated()]))\n",
    "    print(f\"\\nCounts\\n{df.count()}\\n\")\n",
    "    # check for dupe rows\n",
    "    print(f\"\\nValue Counts:\\n{df.value_counts()}\\nNulls:\\n{df.notna()}\\n\")\n",
    "    # examine unique values; look for binary variables and other non-continuous data\n",
    "    print(f\"\\nUnique values:\\n{df.nunique()}\\n\")\n",
    "    # checking for NaN values\n",
    "    print(f\"\\nNulls in cloumns: \\n{df.isna().sum()}\\n\")\n",
    "    # count for whole dataframe\n",
    "    print(f\"Number of nulls in entire dataset: {df.isna().sum().sum()}\\n\")\n",
    "    df.corr()\n",
    "\n",
    "def viz(df):\n",
    "    # Look for zeroes in datasets that may be unreasonable.\n",
    "    fig, axs = plt.subplots(len(df.columns), figsize=(5, 20))\n",
    "    for n, col in enumerate(df.columns):\n",
    "        # print(col)\n",
    "        a = df[col].hist(ax=axs[n])\n",
    "        a.set_title(col)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # cholesterol stood out with about 18% of the values being 0 - visualize\n",
    "    df.Cholesterol.plot(kind = \"hist\", bins = 20, figsize = (8,5))\n",
    "    plt.legend()\n",
    "    plt.savefig(\"images\\cholesterol.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Viz for title page \n",
    "    plt.figure(figsize=(20,20))\n",
    "    sns.displot(df['Age'], color=\"red\", label=\"Age\", kde= True)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"images\\CVD_by_Age.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # look at correlations to target\n",
    "    targetHeatmap = sns.heatmap(\n",
    "        df.corr()[['HeartDisease']].sort_values(by='HeartDisease', ascending=False), \n",
    "        vmin=-0.5, vmax=1, annot=True, cmap='BrBG', fmt = '.2f'\n",
    "        )\n",
    "    fig = targetHeatmap.get_figure()\n",
    "    fig.savefig('images\\heartdiseaseCorr.png')\n",
    "    fig\n",
    "\n",
    "    # Look for outliers in numeric variables\n",
    "    col = ['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak']\n",
    "    num_col = df[col]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize =(10, 10))\n",
    "    fig.patch.set_facecolor('#f1faee')\n",
    "\n",
    "    c = 1\n",
    "    for i in num_col.columns: \n",
    "        plt.subplot(3, 2, c)\n",
    "        plt.boxplot(num_col[i])\n",
    "        plt.title(i, fontsize=12, color='Black')\n",
    "        plt.suptitle('Numeric Variables', fontsize=20)\n",
    "        c = c + 1\n",
    "    fig.savefig('images\\outliers.png')\n",
    "    \n",
    "    # check categorical variables\n",
    "    heart_df = df[df['HeartDisease'] == 1]\n",
    "    fig, ax = plt.subplots(figsize =(10, 10))\n",
    "    fig.patch.set_facecolor('#f1faee')\n",
    "    fig.savefig('images\\categoricalHistograms.png')\n",
    "\n",
    "    j = 1\n",
    "    for i in heart_df.columns: \n",
    "        if heart_df[i].dtypes  == 'object':\n",
    "            plt.subplot(3, 2, j)\n",
    "            sns.countplot(y = heart_df[i], data = heart_df, order=heart_df[i].value_counts().index, palette='Blues_r', edgecolor='black')\n",
    "            #plt.title(i, fontsize=15, color='black')\n",
    "            plt.suptitle('Categorical Variables', fontsize=20)\n",
    "            j = j + 1\n",
    "    print(f\"\\n\\nSeveral visualizations for the final report were generated in the '\\images' folder.  \"\n",
    "        f\"Examine there or in the final report\\n\\n\"\n",
    "        )\n",
    "\n",
    "def dataCleanTransform(df):\n",
    "    # through visualization found 1 zero value for blood pressure = dead\n",
    "    # impute mean value for 0 in the resting BP with the mean for that demographic\n",
    "    df.RestingBP.mask(df.RestingBP == 0, 132, inplace=True)\n",
    "    cleanedDF = df\n",
    "    # encode categorical variables\n",
    "    categorical = ['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope']\n",
    "    encoded = pd.get_dummies(cleanedDF[categorical])\n",
    "    print(encoded.info())\n",
    "    # joing original dataframe and encoded set of features\n",
    "    # create new dataframe and only assign columsn with numerical values\n",
    "    encodedDF = df.drop(columns=categorical)\n",
    "    encodedDF = encodedDF.join(encoded)\n",
    "    print(encodedDF)\n",
    "\n",
    "    print(f\"\\n\\nNew, cleaned data stats: \\n\"\n",
    "        f\"{encodedDF.describe()}\")\n",
    "    # get column names\n",
    "    headers = encodedDF.columns\n",
    "\n",
    "    # Visualize new dataframe with pairplots\n",
    "    # Feature Pair Plots\n",
    "    sns.pairplot(data=encodedDF,\n",
    "        x_vars=headers,\n",
    "        y_vars=headers,\n",
    "        diag_kind='kde'\n",
    "    )\n",
    "    # Too many features to see anything substantial; splitting into two\n",
    "    a = sns.pairplot(data=encodedDF.iloc[:,0:12],diag_kind='kde',corner=True)\n",
    "    a.savefig(\"images\\pairplot.png\")\n",
    "    b = sns.pairplot(data=encodedDF.iloc[:,13:],diag_kind='kde',corner=True)\n",
    "    b.savefig(\"images\\pairplot2.png\")\n",
    "    # b has no useful data, just print a\n",
    "    print(\n",
    "        f\"\\n\\nPair plots \\n\"\n",
    "        f\"{a}\"\n",
    "    )\n",
    "\n",
    "    # Heatmap\n",
    "    fig, ax = plt.subplots(figsize= (10,6))\n",
    "    corrImage = sns.heatmap(df.corr(), vmin=-1, vmax=1, cmap= 'BrBG', annot=True)\n",
    "    fig = corrImage.get_figure()\n",
    "    fig.savefig('images/correlationHeatMap.png')\n",
    "\n",
    "    # The output above is not as clear as it could be. Let's mask some of the output.\n",
    "    # get upper triangle with NumPy method\n",
    "    np.triu(np.ones_like(df.corr()))\n",
    "    # set plot size\n",
    "    plt.figure(figsize=(16,6))\n",
    "    # create heatmap with lower triangle only\n",
    "    upper = np.triu(np.ones_like(df.corr(), dtype=np.bool_))\n",
    "    triangleHeatMap = sns.heatmap(\n",
    "        df.corr(), mask=upper, vmin=-0.5, vmax=1, annot=True, cmap='Blues'\n",
    "        )\n",
    "    fig = triangleHeatMap.get_figure()\n",
    "    fig.savefig('images/TriangleHeatMap.png')\n",
    "\n",
    "    return encodedDF\n",
    "\n",
    "def model(df):\n",
    "    # check counts for heart disease or not, and is it balanced or not?\n",
    "    round(df['HeartDisease'].value_counts()/len(df['HeartDisease'])*100)\n",
    "\n",
    "    # Dummy variables\n",
    "    pd.set_option('display.max_columns', 40)\n",
    "\n",
    "    # Select categorical variables\n",
    "    cat = df.select_dtypes(include=object).columns\n",
    "    df_dummy = pd.get_dummies(df, columns=cat)\n",
    "\n",
    "    inp = df_dummy.drop(columns='HeartDisease')\n",
    "    out = df_dummy['HeartDisease']\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        inp, out, test_size=0.20, random_state=20)\n",
    "    \n",
    "    # Log regression\n",
    "    log_reg = LogisticRegression(solver='liblinear').fit(x_train, y_train)\n",
    "    y_pred_lr = log_reg.predict(x_test)\n",
    "    print('Accuracy score: ', round(accuracy_score(y_test, y_pred_lr), 3))\n",
    "    print('F1 Score: ', round(f1_score(y_test, y_pred_lr), 3))\n",
    "\n",
    "    # input, output var declarations \n",
    "    Y = df_dummy['HeartDisease']\n",
    "    X = df_dummy[['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']]\n",
    "    columns = X.columns\n",
    "    # apply Standardization\n",
    "    scaler = StandardScaler()\n",
    "    X_std = scaler.fit_transform(X)\n",
    "    X_std = pd.DataFrame(X_std, columns = columns)\n",
    "    # Dataset of all dummy columns\n",
    "    df2 = df_dummy.iloc[:,7:]\n",
    "    # Merge the Standardization column with dummy columns\n",
    "    X_nr = X_std.join(df2)\n",
    "\n",
    "    # train test split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        X_nr, Y, test_size=0.20, random_state=20)\n",
    "    \n",
    "    log_reg_nr = LogisticRegression(solver='liblinear').fit(x_train, y_train)\n",
    "    y_pred_lr_nr = log_reg_nr.predict(x_test)\n",
    "    print('Accuracy score: ', round(accuracy_score(y_test, y_pred_lr_nr), 3))\n",
    "    print('F1 Score: ', round(f1_score(y_test, y_pred_lr_nr), 3))\n",
    "\n",
    "    # Confusion Matrix for Logistic RegressionÂ¶\n",
    "    predictedLabels: np.ndarray = log_reg_nr.predict(x_test)\n",
    "    confusionMatrixDF: pd.DataFrame = pd.DataFrame(metrics.confusion_matrix(y_test,predictedLabels),index=['Actual +','Actual -'],columns=['Predicted +', 'Predicted -'])\n",
    "    print(confusionMatrixDF)\n",
    "    print(metrics.classification_report(y_test, y_pred_lr))\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(random_state=20, n_estimators=100)\n",
    " \n",
    "    # x_train, x_test, y_train, y_test\n",
    "    rf = rf.fit(x_train, y_train)\n",
    "    y_pred_rf = rf.predict(x_test)\n",
    "\n",
    "    print('Accuracy score: ', round(accuracy_score(y_test, y_pred_rf), 3))\n",
    "    print('F1 Score: ', round(f1_score(y_test, y_pred_rf), 3))\n",
    "\n",
    "    # Tune the model with grid search\n",
    "    n_estimators = [100, 200, 300]\n",
    "    max_depth = [10, 20, 30]\n",
    "    max_depth.append(None)\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    min_samples_split = [5, 10, 15]\n",
    "    min_samples_leaf = [1, 2]\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    params = {'n_estimators': n_estimators, 'max_features': max_features,\n",
    "            'max_depth': max_depth, 'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\n",
    "\n",
    "    RF = RandomForestClassifier(random_state=20)\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = RF, \n",
    "                            param_grid = params,\n",
    "                            scoring = 'f1',\n",
    "                            cv = 5,\n",
    "                            verbose=0, \n",
    "                            n_jobs=-1)\n",
    "\n",
    "    grid_search.fit(x_train, y_train)\n",
    "    print(\"best score: \", grid_search.best_score_)\n",
    "    print(\"best param: \", grid_search.best_params_)\n",
    "\n",
    "    # apply best params to the RF model\n",
    "    best_para = grid_search.best_params_\n",
    "    rf_2 = RandomForestClassifier(random_state=20, **best_para)\n",
    "                                \n",
    "    rf_2 = rf_2.fit(x_train, y_train)\n",
    "    y_pred_rf_2 = rf_2.predict(x_test)\n",
    "    print(f'\\nReport on Ramdom Forest\\n')\n",
    "    print('Accuracy score: ', round(accuracy_score(y_test, y_pred_rf_2), 3))\n",
    "    print('F1 Score: ', round(f1_score(y_test, y_pred_rf_2), 3))\n",
    "\n",
    "    # Confusion matrix and report for RF\n",
    "    print(metrics.confusion_matrix(y_test, y_pred_rf_2))\n",
    "    print(metrics.classification_report(y_test, y_pred_rf_2))\n",
    "\n",
    "    # predictedLabels: np.ndarray = log_reg_nr.predict(x_test)\n",
    "    confusionMatrixDF: pd.DataFrame = pd.DataFrame(metrics.confusion_matrix(y_test,y_pred_rf_2),index=['Actual +','Actual -'],columns=['Predicted +', 'Predicted -'])\n",
    "    print(confusionMatrixDF)\n",
    "    print(metrics.classification_report(y_test, y_pred_rf_2))\n",
    "\n",
    "    # See what SKlearn says are important features with dummy variables in model\n",
    "    from sklearn.feature_selection import mutual_info_classif\n",
    "    plt.figure(figsize=(15,8))\n",
    "    imp = mutual_info_classif(inp, out)\n",
    "    feature_imp = pd.Series(imp, df_dummy.columns[0:len(df_dummy.columns)-1])\n",
    "    feature_imp = feature_imp.sort_values(ascending=True)\n",
    "    feature_imp.plot(kind = 'barh', color = 'teal')\n",
    "    plt.title(\"Feature importance plot\")\n",
    "    plt.savefig(\"images\\\\featureImportance.png\")\n",
    "\n",
    "    # See impact of using only the top 10 features\n",
    "    best_feat_df = df_dummy[['ST_Slope_Flat','Sex_M','ST_Slope_Down','RestingECG_ST','ExerciseAngina_N', \n",
    "                            'Oldpeak','ChestPainType_ASY','MaxHR','Sex_F','Cholesterol','HeartDisease']]\n",
    "\n",
    "    inp_feat = best_feat_df.drop(columns='HeartDisease')\n",
    "    out_feat = best_feat_df['HeartDisease']\n",
    "\n",
    "    x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(\n",
    "        inp_feat, out_feat, test_size=0.20, random_state=20)\n",
    "    \n",
    "    rf_feat = RandomForestClassifier(random_state=20, n_estimators=100)\n",
    " \n",
    "    rf_feat = rf_feat.fit(x_train_2, y_train_2)\n",
    "    y_pred_rf_feat = rf_feat.predict(x_test_2)\n",
    "    print(f'\\nReport on Ramdom Forest with only top 10 feautres\\n')\n",
    "    print('Accuracy score: ', round(accuracy_score(y_test_2, y_pred_rf_feat), 3))\n",
    "    print('F1 Score: ', round(f1_score(y_test_2, y_pred_rf_feat), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory images failed; it already exists.\n",
      "Creation of the directory data failed; it already exists.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'describe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\BRUNSO~1\\AppData\\Local\\Temp/ipykernel_23188/3942543764.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\BRUNSO~1\\AppData\\Local\\Temp/ipykernel_23188/3942543764.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mmakeDir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mdataFrame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataLoad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0meda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mviz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mnewDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataCleanTransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\BRUNSO~1\\AppData\\Local\\Temp/ipykernel_23188/2126418878.py\u001b[0m in \u001b[0;36meda\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0meda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# check for dupes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'describe'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3.97\n",
    "\n",
    "# Dependencies\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def main():\n",
    "    makeDir()\n",
    "    dataFrame = dataLoad()\n",
    "    eda(dataFrame)\n",
    "    viz(dataFrame)\n",
    "    newDF = dataCleanTransform(dataFrame)\n",
    "\n",
    "    ''' Comment out for now'''\n",
    "    # # This library captures many EDA elements\n",
    "    # # https://pandas-profiling.github.io/pandas-profiling/docs/master/index.html\n",
    "    # profile = ProfileReport(dataFrame, title=\"Cardiovascular Disease Data Profiling Report\", explorative=True)\n",
    "    # # export for report\n",
    "    # profile.to_file(\"CVD.html\")\n",
    "    # # print to screen in interactive frame\n",
    "    # # 3.1 widgets() is broken; regress to 3.0 for widgets or use to_notebook_iframe (CAO: Nov 2021)\n",
    "    # # Selecting alternative path since user action required to rollback version\n",
    "    # profile.to_notebook_iframe()\n",
    "    '''end comment'''\n",
    "    print(f\"\\n\\nNow running models to train and test data with new dataset.\\n\")\n",
    "    model(newDF)\n",
    "    cleanedDataFile(newDF)     \n",
    "    # run other notebooks in library\n",
    "    # %run ./eda.ipynb\n",
    "    # %run ./models.ipynb\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
